import os
import time
import torch
import requests
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# ========== –ù–ê–°–¢–†–û–ô–ö–ò –û–ö–†–£–ñ–ï–ù–ò–Ø ==========
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["TRANSFORMERS_VERBOSITY"] = "error" 
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"  # –û—Ç–∫–ª—é—á–∞–µ–º —É—Å–∫–æ—Ä–µ–Ω–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É

print("üöÄ –¢–ï–°–¢–û–í–´–ô –°–¢–ï–ù–î - –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –° –£–í–ï–õ–ò–ß–ï–ù–ù–´–ú–ò –¢–ê–ô–ú–ê–£–¢–ê–ú–ò")
print("=" * 60)

# ========== –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´ ==========
def system_diagnostics():
    print("üìä –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´:")
    try:
        import shutil
        import psutil
        
        # –î–∏—Å–∫
        disk_free = shutil.disk_usage('/').free / 1024**3
        print(f"   üíæ –°–≤–æ–±–æ–¥–Ω–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ: {disk_free:.1f} GB")
        
        # –ü–∞–º—è—Ç—å
        mem = psutil.virtual_memory()
        print(f"   üß† –î–æ—Å—Ç—É–ø–Ω–æ –ø–∞–º—è—Ç–∏: {mem.available / 1024**3:.1f} GB ({mem.percent}% –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)")
        
        # –°–µ—Ç—å
        print(f"   üåê –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø –∫ HuggingFace...")
        response = requests.get("https://huggingface.co", timeout=10)
        print(f"   ‚úÖ HuggingFace –¥–æ—Å—Ç—É–ø–µ–Ω (—Å—Ç–∞—Ç—É—Å: {response.status_code})")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: {e}")

system_diagnostics()
print("-" * 40)

# ========== –ö–ê–°–¢–û–ú–ù–´–ï –¢–ê–ô–ú–ê–£–¢–´ ==========
print("‚è±Ô∏è  –ù–ê–°–¢–†–ê–ò–í–ê–ï–ú –¢–ê–ô–ú–ê–£–¢–´...")

from transformers.utils.hub import http_get

original_http_get = http_get

def custom_http_get(url, temp_file, proxies=None, resume_size=0, headers=None, timeout=180):
    """–£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤"""
    print(f"   üì• –ó–∞–≥—Ä—É–∂–∞–µ–º: {url.split('/')[-1]}")
    start_time = time.time()
    
    try:
        result = original_http_get(
            url, temp_file, 
            proxies=proxies, 
            resume_size=resume_size, 
            headers=headers, 
            timeout=timeout
        )
        download_time = time.time() - start_time
        print(f"   ‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {download_time:.1f}—Å–µ–∫")
        return result
    except Exception as e:
        download_time = time.time() - start_time
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ {download_time:.1f}—Å–µ–∫: {e}")
        raise

# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π http_get
import transformers.modeling_utils
import transformers.tokenization_utils_base
transformers.modeling_utils.http_get = custom_http_get
transformers.tokenization_utils_base.http_get = custom_http_get

print("‚úÖ –¢–∞–π–º–∞—É—Ç—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã (180 —Å–µ–∫—É–Ω–¥)")
print("-" * 40)

# ========== –¢–ï–°–¢ –ó–ê–ì–†–£–ó–ö–ò ==========
def test_model_loading():
    print("üß™ –¢–ï–°–¢ –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò:")
    
    # 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print("1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
    tokenizer_start = time.time()
    try:
        tokenizer = GPT2Tokenizer.from_pretrained(
            "sberbank-ai/rugpt3small_based_on_gpt2",
            local_files_only=False
        )
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer_time = time.time() - tokenizer_start
        print(f"   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {tokenizer_time:.1f}—Å–µ–∫")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        return False, None, None

    # 2. –ú–æ–¥–µ–ª—å
    print("2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (~500MB)...")
    model_start = time.time()
    try:
        model = GPT2LMHeadModel.from_pretrained(
            "sberbank-ai/rugpt3small_based_on_gpt2",
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True,
            force_download=True,      # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º
            resume_download=False,    # –ó–∞–ø—Ä–µ—â–∞–µ–º –¥–æ–∫–∞—á–∫—É
            local_files_only=False
        )
        model.eval()
        model_time = time.time() - model_start
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {model_time:.1f}—Å–µ–∫")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False, None, None

    return True, tokenizer, model

# ========== –¢–ï–°–¢ –ì–ï–ù–ï–†–ê–¶–ò–ò ==========
def test_generation(tokenizer, model):
    print("3. –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...")
    
    test_queries = [
        "–ü—Ä–∏–≤–µ—Ç",
        "–ö–∞–∫ –¥–µ–ª–∞?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"   {i}. –ó–∞–ø—Ä–æ—Å: '{query}'")
        try:
            inputs = tokenizer.encode(query, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_new_tokens=20,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            clean_response = response.replace(query, "").strip()
            print(f"      –û—Ç–≤–µ—Ç: '{clean_response}'")
            
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return False
    
    return True

# ========== –ó–ê–ü–£–°–ö –¢–ï–°–¢–ê ==========
if __name__ == "__main__":
    print("üéØ –ó–ê–ü–£–°–ö–ê–ï–ú –¢–ï–°–¢...")
    print("‚ö†Ô∏è  –ï—Å–ª–∏ –∑–∞–≤–∏—Å–Ω–µ—Ç - –∂–¥—ë–º –¥–æ 3 –º–∏–Ω—É—Ç –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏")
    print("=" * 60)
    
    total_start = time.time()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    success, tokenizer, model = test_model_loading()
    
    if success and tokenizer and model:
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        gen_success = test_generation(tokenizer, model)
        
        total_time = time.time() - total_start
        
        if gen_success:
            print("=" * 60)
            print(f"üéâ –¢–ï–°–¢ –ü–†–û–ô–î–ï–ù –£–°–ü–ï–®–ù–û!")
            print(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
            print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
        else:
            print("=" * 60)
            print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å, –Ω–æ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π")
    else:
        print("=" * 60)
        print("üí• –¢–ï–°–¢ –ü–†–û–í–ê–õ–ï–ù!")
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
        
    print("=" * 60)