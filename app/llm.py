from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch


class LLMReportGenerator:
    _instance = None
    _initialized = False
    
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, model_name="Vikhrmodels/Vikhr-YandexGPT-5-Lite-8B-it_MLX-8bit"):
        if LLMReportGenerator._initialized:
            return
            
        print(f"‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_name}...")
        print("üì• –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                device_map="cpu",
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                load_in_8bit=True,          # 8-–±–∏—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            )
            
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            self.model.eval()
            
            # –°–æ–∑–¥–∞—ë–º –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —É–ø—Ä–æ—â—ë–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã
            self.pipe = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                torch_dtype=torch.float16,
                device_map="cpu"
            )
            
            self.history = []
            print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
            LLMReportGenerator._initialized = True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {str(e)}")
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–ª–∞–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            LLMReportGenerator._initialized = False
            raise
    
    def generate_response(self, user_input):
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –ø–æ–¥—Ö–æ–¥—è—â–µ–º –¥–ª—è –º–æ–¥–µ–ª–∏
            if self.history:
                conversation = "\n".join([f"{'User' if i % 2 == 0 else 'Assistant'}: {msg['content']}" 
                                        for i, msg in enumerate(self.history[-4:])])
                prompt = f"{conversation}\nUser: {user_input}\nAssistant:"
            else:
                prompt = f"User: {user_input}\nAssistant:"
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π –º–æ–¥–µ–ª–∏
            outputs = self.pipe(
                prompt,
                max_new_tokens=256,        # –£–≤–µ–ª–∏—á–∏–º –Ω–µ–º–Ω–æ–≥–æ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                top_k=50,                  # –î–æ–±–∞–≤–ª—è–µ–º top_k –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.eos_token_id,
                num_return_sequences=1,
                eos_token_id=self.tokenizer.eos_token_id,
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–≤–µ—Ç
            full_text = outputs[0]['generated_text']
            response = full_text.replace(prompt, "").strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—à–Ω–µ–≥–æ
            if "\nUser:" in response:
                response = response.split("\nUser:")[0]
            if "<|endoftext|>" in response:
                response = response.split("<|endoftext|>")[0]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
            self.history.append({"role": "user", "content": user_input})
            self.history.append({"role": "assistant", "content": response})
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
            if len(self.history) > 10:
                self.history = self.history[-10:]
                
            return response
            
        except Exception as e:
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
    
    def clear_history(self):
        """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞"""
        self.history = []