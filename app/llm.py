from huggingface_hub import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
from time import time as time_now


# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'


class LLMReportGenerator:
    _instance = None
    _initialized = False
    
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, model_name="Qwen/Qwen3-4B"):
        if LLMReportGenerator._initialized:
            return
            
        print(f"‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_name}...")
        print("üì• –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...")
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ mirror
        local_dir = snapshot_download(
            repo_id=model_name,
            local_dir=f"./models/{model_name.replace('/', '_')}",
            endpoint="https://hf-mirror.com"
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.tokenizer = AutoTokenizer.from_pretrained(
            local_dir,
            trust_remote_code=True
        )
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ë–ï–ó device_map –¥–ª—è CPU
        self.model = AutoModelForCausalLM.from_pretrained(
            local_dir,
            torch_dtype=torch.float32,
            device_map="cpu",  # –£–±–∏—Ä–∞–µ–º —ç—Ç—É —Å—Ç—Ä–æ–∫—É
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # –Ø–≤–Ω–æ –ø–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU
        # self.model = self.model.to('cpu')
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        self.model.eval()
        
        self.history = []
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        LLMReportGenerator._initialized = True
    
    def generate_response(self, promt):
        try:
            start_time = time_now()
            messages = [
                {"role": "user", "content": promt}
            ]
            
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=False
            )
            
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=32768
            )
            output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
            response = self.tokenizer.decode(output_ids[:], skip_special_tokens=True).strip("\n")
            
            end_time = time_now()
            
            return {
                "response": response,
                "time": end_time - start_time
            }
            
        except Exception as e:
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
    