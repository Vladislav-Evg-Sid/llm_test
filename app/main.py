from fastapi import FastAPI

from llm_plug import LLMReportGenerator
from py_models import LLLMResponse

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä FastAPI
app = FastAPI(title="Heavy Class Demo")


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LlamaChatbot –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    print("üîÑ –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")
    # –ü—Ä–æ—Å—Ç–æ –ø–æ–ª—É—á–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä - –æ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è
    heavy_instance = LLMReportGenerator()
    print("‚úÖ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –∫ —Ä–∞–±–æ—Ç–µ")


@app.post("/generate_text")
async def generate_endpoint(user_request: str):
    llm = LLMReportGenerator()
    result = LLLMResponse()
    try:
        result.text = llm.generate_response(user_request)
    except:
        result.text = "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
    return result
