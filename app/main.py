from fastapi import FastAPI, HTTPException
from pathlib import Path
from dotenv import load_dotenv
import os

from py_models import LLLMResponse
import asyncio

env_path = Path(__file__).resolve().parents[0] / ".env"
load_dotenv(dotenv_path=env_path)

if os.getenv('CURRENT_DEVICE') == "server":
    from llm import LLMReportGenerator
else:
    from llm_plug import LLMReportGenerator

app = FastAPI(title="Heavy Class Demo")

@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    print("üîÑ –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")
    
    loop = asyncio.get_event_loop()
    await loop.run_in_executor(
        None, 
        lambda: LLMReportGenerator()
    )
    print("‚úÖ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –∫ —Ä–∞–±–æ—Ç–µ")

@app.post("/generate_text")
async def generate_endpoint(user_request: str):
    result = LLLMResponse()
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω–≥–ª—Ç–æ–Ω - –≤—Å–µ–≥–¥–∞ –ø–æ–ª—É—á–∞–µ–º —Ç–æ—Ç –∂–µ —ç–∫–∑–µ–º–ø–ª—è—Ä
        llm = LLMReportGenerator()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        response = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: llm.generate_response(user_request)
        )
        result.text = response
    except Exception as e:
        result.text = f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
    
    return result